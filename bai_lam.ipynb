{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2901038 , -0.2898787 ,  0.4169351 , ...,  0.5113254 ,\n",
       "        -0.5972742 , -1.138472  ],\n",
       "       [-1.308045  ,  0.9506664 ,  2.3402    , ..., -0.7167358 ,\n",
       "        -1.000471  , -0.2341897 ],\n",
       "       [ 2.561394  ,  2.765394  , -2.260592  , ...,  0.9035186 ,\n",
       "        -0.4179576 , -0.6470901 ],\n",
       "       ...,\n",
       "       [-0.2854436 , -0.05352458, -0.6313937 , ...,  0.3718548 ,\n",
       "        -0.7246644 ,  0.7017136 ],\n",
       "       [ 0.4465973 , -0.205611  , -1.250274  , ...,  1.175337  ,\n",
       "        -0.4805564 , -0.08237529],\n",
       "       [-0.07976314,  0.44587   , -1.285894  , ..., -0.178924  ,\n",
       "         0.125074  , -0.7589334 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"./word2vec/W2V_150.txt\", \"r\")\n",
    "arr = []\n",
    "for i in f:\n",
    "    arr.append(i.rstrip())\n",
    "\n",
    "word_arr = []\n",
    "vector_arr =[]\n",
    "\n",
    "for i in range(2,len(arr),1):\n",
    "    word = arr[i].split(\"  \")[0]\n",
    "    vector = arr[i].split(\"  \")[1]\n",
    "    vector = vector.split(\" \")\n",
    "    vector = [ float(x) for x in vector ]\n",
    "    \n",
    "    word_arr.append(word)\n",
    "    vector_arr.append(vector)\n",
    "\n",
    "vector_arr = np.array(vector_arr)\n",
    "vector_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.22195845e-01,  7.79282376e-02, -1.76763330e-01,  1.44790641e-02,\n",
       "       -7.65938136e-02,  1.32186088e-01,  6.07686724e-02, -1.95175371e-01,\n",
       "       -1.55522183e-01, -1.41611330e-01,  5.35632842e-02, -6.63222565e-02,\n",
       "       -1.61330832e-01, -5.42867159e-02, -1.73662407e-02, -3.54079710e-01,\n",
       "       -2.07915396e-02, -5.54753013e-02, -7.30031394e-03,  1.95452326e-01,\n",
       "       -1.47048386e-01, -7.29751845e-02,  2.59170160e-01, -2.54200031e-01,\n",
       "        1.03882139e-01,  1.90335939e-01,  2.23196176e-01, -7.40311337e-03,\n",
       "       -1.79738594e-01, -1.89193189e-01,  8.83329702e-02,  1.39421657e-01,\n",
       "        2.03971636e-01, -1.40089046e-01,  2.43129156e-02, -2.74665429e-01,\n",
       "       -1.03753860e-01, -2.41940335e-01,  2.10875167e-02, -1.36225529e-01,\n",
       "       -2.13684185e-01, -1.37370405e-01,  5.57762391e-02, -4.00039790e-01,\n",
       "       -1.55477409e-01, -2.01678499e-01,  6.08960266e-04, -9.03864492e-02,\n",
       "       -1.54112455e-01, -1.09130318e-02, -1.50455538e-01, -1.44178848e-02,\n",
       "        7.20080316e-02,  4.57710872e-03,  3.61159253e-01, -1.53633596e-01,\n",
       "        1.76862113e-01,  5.03376050e-02,  1.24211834e-02,  1.38022736e-01,\n",
       "        2.18214905e-01, -3.36070822e-02, -7.90850992e-02, -3.02669392e-01,\n",
       "        1.67133016e-01,  1.09092811e-01,  7.14681546e-02, -1.51711731e-01,\n",
       "       -2.33613418e-01,  3.87903003e-02,  5.30256402e-02,  2.37479106e-01,\n",
       "        2.20806927e-01, -2.18417659e-01,  1.04375249e-01, -7.91764583e-02,\n",
       "        2.21098819e-02,  6.10853192e-02,  2.70443247e-01,  5.50775889e-02,\n",
       "       -1.63150916e-01,  2.73481141e-02,  2.35050260e-01,  6.19876484e-02,\n",
       "       -1.96228081e-01,  1.28080553e-01,  2.53384498e-01, -1.84758410e-01,\n",
       "       -1.62459241e-01,  2.65053264e-02,  1.35078437e-01,  2.45838127e-01,\n",
       "        5.83449628e-02,  3.36883198e-02,  4.04342321e-01,  1.89503298e-01,\n",
       "        1.08459360e-01, -3.33514199e-02,  1.33802384e-01,  2.59525157e-02,\n",
       "        1.57904200e-01, -3.86335807e-01,  2.75367104e-01, -2.02138074e-02,\n",
       "       -1.56696914e-01,  1.53716600e-02, -1.41683861e-01, -5.95157870e-02,\n",
       "        2.62258762e-01, -4.62295952e-02,  6.76566225e-02, -4.98496629e-04,\n",
       "        8.86259545e-02, -1.32765181e-01, -3.81633910e-01, -1.49238145e-01,\n",
       "       -3.55265272e-02,  1.18330043e-01, -2.65795358e-01, -1.33744343e-02,\n",
       "       -1.41228098e-01, -7.89905305e-02, -9.39332603e-02,  1.31166242e-01,\n",
       "        1.50112679e-01,  7.78680070e-03,  2.23504319e-01, -1.95678353e-02,\n",
       "        1.37968211e-01, -1.64028632e-01,  2.05244835e-01,  7.38640793e-03,\n",
       "       -1.90432845e-01, -1.20466946e-01,  1.94270665e-02,  8.08565833e-02,\n",
       "        2.46287227e-01, -2.83881969e-01,  1.03225340e-01, -4.18990068e-02,\n",
       "       -3.28840818e-02,  1.00364261e-01,  2.45181692e-03,  1.54463094e-01,\n",
       "       -1.85746206e-04, -3.53199019e-01, -1.23780835e-01,  8.75130684e-02,\n",
       "       -1.93044285e-01, -1.23626667e-02])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_of_word_unknow = np.mean(vector_arr, axis=0)\n",
    "def embeding_word(word):\n",
    "    try:\n",
    "        return vector_arr[word_arr.index(word)]\n",
    "    except:\n",
    "        return vector_of_word_unknow\n",
    "#         return np.ones(150)\n",
    "    \n",
    "embeding_word(\"chasdasda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2898984440277117"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosin_2_word(word1,word2):\n",
    "    vector1 = embeding_word(word1)\n",
    "    vector2 = embeding_word(word2)\n",
    "    return (1-distance.cosine(vector1,vector2))\n",
    "        \n",
    "cosin_2_word(\"cha\",\"malskjdáº¹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "visim_400_file = open(\"./datasets/ViSim-400/Visim-400.txt\", \"r\")\n",
    "visim_arr = []\n",
    "for i in visim_400_file:\n",
    "    visim_arr.append(i.rstrip())\n",
    "\n",
    "visim_arr.pop(0)\n",
    "visim_arr\n",
    "\n",
    "word_1_arr = []\n",
    "word_2_arr = []\n",
    "sim_1_arr = []\n",
    "sim_2_arr = []\n",
    "std_arr=[]\n",
    "\n",
    "for i in visim_arr:\n",
    "    i = i.split(\"\\t\")\n",
    "    word_1_arr.append(i[0])\n",
    "    word_2_arr.append(i[1])\n",
    "    sim_1_arr.append(float(i[3]))\n",
    "    sim_2_arr.append(float(i[4]))\n",
    "    std_arr.append(float(i[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.004912339469669957,\n",
       " 0.08252318329211772,\n",
       " 0.27708595986827755,\n",
       " 0.17679862835626714,\n",
       " 0.2580890432574774,\n",
       " 0.40236612919430603,\n",
       " 0.46300840201407223,\n",
       " 0.2569470088996123,\n",
       " 0.18519202240211619,\n",
       " 0.2230696045254854,\n",
       " 0.5348913029963392,\n",
       " 0.07770108930396213,\n",
       " 0.26146911020272934,\n",
       " 0.07111606873203247,\n",
       " 0.6438835401838533,\n",
       " 0.3610325527728171,\n",
       " 0.059602370849219266,\n",
       " 0.23895503536998697,\n",
       " 0.16647413914742726,\n",
       " -0.17284007209619467,\n",
       " 0.6417888034717881,\n",
       " 0.28753527791057565,\n",
       " 0.38819716944409455,\n",
       " 0.25596472879600973,\n",
       " 0.48043194238556175,\n",
       " 0.4724262988742084,\n",
       " 0.6624093852340639,\n",
       " 0.22523314998167998,\n",
       " 0.17589195648989997,\n",
       " 0.7340683342360695,\n",
       " -0.01360954451948393,\n",
       " 0.6878254420258124,\n",
       " 0.5964044436335486,\n",
       " 0.3754441422124708,\n",
       " 0.3856135822050447,\n",
       " 0.23305646417105508,\n",
       " -0.03025713683863529,\n",
       " 0.09239815053322087,\n",
       " 0.4812901949758728,\n",
       " 0.4430288482597331,\n",
       " 0.42764020562142635,\n",
       " 0.3435372146285758,\n",
       " 0.47775360404332456,\n",
       " 0.16598255399110184,\n",
       " 1.0,\n",
       " 0.3518800216281459,\n",
       " 0.18436466221055814,\n",
       " 0.5372508509460165,\n",
       " 0.20654491300805344,\n",
       " 0.17018185671127573,\n",
       " -0.09567834469126146,\n",
       " 0.4058112852321534,\n",
       " 0.17700723858902045,\n",
       " 0.09295287557078624,\n",
       " 0.6561799539297559,\n",
       " 0.1591479363526458,\n",
       " 0.2573998625832936,\n",
       " 0.2518184385854666,\n",
       " 0.3138728888635596,\n",
       " 0.3079313269934212,\n",
       " 0.14150346138857595,\n",
       " 0.3536777035092057,\n",
       " 0.16630008250549988,\n",
       " 0.2155590695087991,\n",
       " 0.1759844731109964,\n",
       " 0.49028561544494864,\n",
       " 0.12265149004992304,\n",
       " 0.09024219330167871,\n",
       " -0.009955192323517714,\n",
       " 0.18305778374734583,\n",
       " 0.5501337928711713,\n",
       " 0.21009877021166767,\n",
       " 0.05442648829061236,\n",
       " 0.04305196387475296,\n",
       " 0.19459174867201612,\n",
       " 0.5530819388468131,\n",
       " 0.5396216352705175,\n",
       " 0.44477745011189884,\n",
       " 1.0,\n",
       " 0.18252869545755523,\n",
       " 0.1884327286416303,\n",
       " 0.4305675773234774,\n",
       " 0.728347957294409,\n",
       " 0.48128296649134494,\n",
       " 0.06165375134763973,\n",
       " 0.50182926552691,\n",
       " 0.49719492878886484,\n",
       " 0.2640552460341623,\n",
       " -0.008235419222734741,\n",
       " 0.0016070145754620846,\n",
       " 0.3381737506042728,\n",
       " 0.43346360554125285,\n",
       " 0.16435135139844603,\n",
       " 0.3489107092225543,\n",
       " 0.28764478896895396,\n",
       " 0.3992949727831533,\n",
       " 0.27706866745878234,\n",
       " 0.4727960461441949,\n",
       " 0.2604368094017768,\n",
       " 0.3806990925318219,\n",
       " 0.6520561739956902,\n",
       " 0.2429446412966989,\n",
       " 0.39575242059890214,\n",
       " 0.037029307474394146,\n",
       " 0.425593706376884,\n",
       " 0.3002582475972697,\n",
       " 0.117888186841136,\n",
       " 0.11930273869880292,\n",
       " 0.14183917722562245,\n",
       " 0.10001389172491248,\n",
       " -0.17340856215402978,\n",
       " -0.019664365856755905,\n",
       " 0.43308509164042475,\n",
       " 0.23889655843263058,\n",
       " 0.3924741581975577,\n",
       " 0.4791904396921671,\n",
       " 0.2310026808217749,\n",
       " 0.39499832898781273,\n",
       " 0.36495798502767895,\n",
       " 0.5270150819554982,\n",
       " 0.1400762441855683,\n",
       " -0.04568560756811624,\n",
       " -0.04944032913153995,\n",
       " 0.2860688139946971,\n",
       " 0.6348067524315942,\n",
       " 0.5036435709308654,\n",
       " 0.15002384579859185,\n",
       " 0.07522659536547083,\n",
       " 0.181654903465979,\n",
       " 0.15451682858848514,\n",
       " -0.03716774487133834,\n",
       " 0.37012652970055826,\n",
       " 0.28601100050751826,\n",
       " 0.09990871301551096,\n",
       " 0.296198149952809,\n",
       " 0.43431450273571603,\n",
       " 0.11986410627414434,\n",
       " 0.22222659830449643,\n",
       " 0.08176410293427971,\n",
       " 0.2815387476802429,\n",
       " 0.16802562529114884,\n",
       " 0.11699808527980937,\n",
       " -0.01182905114723476,\n",
       " 0.07773202880372376,\n",
       " 0.12892002412737336,\n",
       " 0.22828985638912025,\n",
       " 0.2665077694367288,\n",
       " 0.3117406057866714,\n",
       " 0.557049383355903,\n",
       " 0.3462440983410393,\n",
       " 0.008037526967851982,\n",
       " 0.0420429607431857,\n",
       " 0.21051430924976455,\n",
       " 0.46645676570529293,\n",
       " 0.2602070836579301,\n",
       " 0.46916483596133274,\n",
       " 0.05814091735576743,\n",
       " 0.4903041567774524,\n",
       " 0.2580890432574774,\n",
       " 0.2369407469699243,\n",
       " 0.1438635231004214,\n",
       " 0.5766044993249184,\n",
       " 0.3821435174904365,\n",
       " 0.32032802076548217,\n",
       " 0.5749492271691554,\n",
       " 0.19389669564397183,\n",
       " 0.2416479539708365,\n",
       " 0.5106627798317874,\n",
       " 0.18926458543469316,\n",
       " 0.20940313081938844,\n",
       " -0.029710370501402794,\n",
       " 0.2837016049614861,\n",
       " 0.12074527931992807,\n",
       " 0.017015264264734542,\n",
       " 0.3598694592527043,\n",
       " 0.2651261938064957,\n",
       " 0.3397885209729339,\n",
       " 0.35946235162793316,\n",
       " 0.4219105066378003,\n",
       " 0.8614688965162296,\n",
       " -0.03336303444684785,\n",
       " 0.12090203297460689,\n",
       " 0.31479701144804784,\n",
       " 0.0038128645074774203,\n",
       " 0.058758192228966166,\n",
       " 0.3968391917193709,\n",
       " 0.12428570197414746,\n",
       " 0.19672622271313023,\n",
       " 0.25933155773078487,\n",
       " 0.23267892254977174,\n",
       " 0.05538533879963636,\n",
       " -0.021294972294592096,\n",
       " 0.15980115685287788,\n",
       " 0.14562126981231704,\n",
       " 0.39828525430318873,\n",
       " 0.18902841445299534,\n",
       " 0.3474293225699694,\n",
       " 0.2497490291208706,\n",
       " 0.6089198433181582,\n",
       " 0.6755826366795625,\n",
       " 0.05314055481261115,\n",
       " 0.10054934907463342,\n",
       " 0.3232909354803847,\n",
       " 0.1483878742775997,\n",
       " 0.570699646165023,\n",
       " 0.20133470150686894,\n",
       " -0.005103764065359728,\n",
       " 0.2526270231473787,\n",
       " 0.045425622535701926,\n",
       " 0.1485309587805349,\n",
       " 0.42687936528822523,\n",
       " 0.6700650441380147,\n",
       " 0.08740919551694981,\n",
       " 0.3141633263083572,\n",
       " 0.06264508541390945,\n",
       " 0.28823654399925047,\n",
       " 0.12474992364601212,\n",
       " 0.3452384366954806,\n",
       " 0.10538504266032922,\n",
       " 0.17679103686899356,\n",
       " 0.07106153794008307,\n",
       " -0.08150806029953683,\n",
       " 0.3356669673410032,\n",
       " 0.2332012292070551,\n",
       " 0.22137551879886286,\n",
       " 0.47463479965705146,\n",
       " 0.09329309110204231,\n",
       " 0.3203891892969699,\n",
       " 0.44064836024327436,\n",
       " 0.8422977453212419,\n",
       " 0.49757481319560104,\n",
       " 0.4115581048163304,\n",
       " 0.03402576743432384,\n",
       " 0.0309784792464729,\n",
       " 0.08605219900956884,\n",
       " 0.45581411434867436,\n",
       " 0.07438552352594874,\n",
       " 0.12071931551226445,\n",
       " 0.3909738620022827,\n",
       " 0.22687194401456257,\n",
       " 0.2182430752414123,\n",
       " 0.08913522264813223,\n",
       " 0.2990252010643608,\n",
       " 0.3393090784679019,\n",
       " 0.7741543992805754,\n",
       " 0.3645971891337768,\n",
       " 0.38243149843243907,\n",
       " 0.06499285201502492,\n",
       " 0.041712493805655004,\n",
       " 0.14642503430199727,\n",
       " 0.25190297932721695,\n",
       " 0.18257703468211917,\n",
       " 0.11202609620054438,\n",
       " 0.055979056308360975,\n",
       " 0.018338220644070424,\n",
       " 0.10625734688523503,\n",
       " 0.6423954310858541,\n",
       " 0.5502537205294664,\n",
       " 0.15856256685433023,\n",
       " 0.30474690142029737,\n",
       " 0.35563124319080797,\n",
       " 0.6408591579368886,\n",
       " 0.10812724165852339,\n",
       " 0.5719292130062954,\n",
       " 0.15468965044907834,\n",
       " 0.04145193685409565,\n",
       " 0.10091694820831953,\n",
       " 0.4987182279416986,\n",
       " 0.19975550471725922,\n",
       " 0.11630711350656076,\n",
       " 0.2906393488773251,\n",
       " 0.14819665087209488,\n",
       " 0.10802345785899914,\n",
       " 0.08307659943912193,\n",
       " 0.5963759133200228,\n",
       " 0.40069460609952456,\n",
       " 0.4831838083897517,\n",
       " 0.0869298285463398,\n",
       " 0.4154167385619685,\n",
       " 0.19345520749942513,\n",
       " 0.02805270675860161,\n",
       " 0.30755466764509976,\n",
       " 0.10574808894699206,\n",
       " 0.3174415682631986,\n",
       " 0.09141503336475965,\n",
       " 0.012591047143755474,\n",
       " 0.23699363326882117,\n",
       " 0.0035618958136457435,\n",
       " 0.10090909433832695,\n",
       " 0.10504266823172614,\n",
       " 0.16571494195645498,\n",
       " 0.7283213214108306,\n",
       " 0.687693273029439,\n",
       " 0.5077456024563395,\n",
       " 0.2140835028640664,\n",
       " -0.013589524766159489,\n",
       " 0.5461646248126156,\n",
       " 0.33919365521435485,\n",
       " 0.1212163285207215,\n",
       " 0.3751939726641791,\n",
       " 0.3039713821404869,\n",
       " 0.3458204697154301,\n",
       " 0.06940581189870454,\n",
       " 0.19862973632444048,\n",
       " 0.43775166680046285,\n",
       " 0.4213006540115999,\n",
       " 0.6075960108364697,\n",
       " 0.32489756762342714,\n",
       " 0.1112194069190986,\n",
       " 0.4037583366419032,\n",
       " 0.38944377934967556,\n",
       " 0.7733680092570684,\n",
       " 0.19783202617630358,\n",
       " 0.0943707491060577,\n",
       " 0.5305708447274694,\n",
       " 0.08348869272935444,\n",
       " 0.13855356790222584,\n",
       " 0.16362530474947723,\n",
       " 0.11444190475563798,\n",
       " 0.15933520760572362,\n",
       " 0.15172684174398732,\n",
       " 0.3396688306948408,\n",
       " 0.18473887648929466,\n",
       " 0.45093044850844466,\n",
       " 0.15508711627027516,\n",
       " 0.5284669282716832,\n",
       " 0.1992037674039463,\n",
       " 0.20046350783200317,\n",
       " 0.03124777346515506,\n",
       " 0.44057553236059877,\n",
       " 0.5834579031319869,\n",
       " 0.3150380450984238,\n",
       " 0.13102407921040848,\n",
       " 0.3391485677790276,\n",
       " 0.299426422462176,\n",
       " 0.25047609054912046,\n",
       " -0.11709220841298307,\n",
       " 0.007990260459697529,\n",
       " 0.05150617212689046,\n",
       " 0.12210294605784289,\n",
       " 0.13180365946970285,\n",
       " 0.8179892267593377,\n",
       " 0.15042709634133444,\n",
       " 0.19914344122783612,\n",
       " 0.1480772554325741,\n",
       " 0.1900175709520523,\n",
       " 0.12946731392230693,\n",
       " 0.32445784354190854,\n",
       " -0.0008110941303698382,\n",
       " 0.1844720942077065,\n",
       " 0.16119306934809963,\n",
       " 0.005273336871372658,\n",
       " -0.011613802522149719,\n",
       " 0.15949929905506444,\n",
       " 0.3329660792334814,\n",
       " 0.17995018188629508,\n",
       " 0.25811458928916675,\n",
       " 0.06362501672097443,\n",
       " 0.4074055863436804,\n",
       " 0.5909680719076433,\n",
       " 0.13353498403895525,\n",
       " 0.25218032550202985,\n",
       " 0.4621548455520126,\n",
       " -0.08608104825305274,\n",
       " 0.14359299098434875,\n",
       " 0.9057290306863826,\n",
       " 0.3576096489593774,\n",
       " 0.6325065472381484,\n",
       " 0.6970440205524104,\n",
       " 0.1792190013315711,\n",
       " 0.372503751184449,\n",
       " 0.1642864044630158,\n",
       " 0.2963445022396367,\n",
       " 0.6541988461158454,\n",
       " 0.437246974945656,\n",
       " 0.08713397403587109,\n",
       " 0.3410369530265618,\n",
       " 0.29461659219600633,\n",
       " 0.03854739022579878,\n",
       " 0.20622362094102542,\n",
       " 0.24376703521286547,\n",
       " 0.5151665388049849,\n",
       " 0.4095160578135807,\n",
       " 0.19994660282373244,\n",
       " 0.6742159346587002,\n",
       " 0.2306157487097722,\n",
       " 0.1835720811240651,\n",
       " 0.13696782538735874,\n",
       " 0.1845901522102158,\n",
       " 0.4767118176565459,\n",
       " 0.05671383013487896,\n",
       " 0.470767001732449,\n",
       " 0.4944465344278004,\n",
       " 0.2743758563848939,\n",
       " 0.2580890432574774,\n",
       " 0.17049420746310417,\n",
       " 0.1350079186615406,\n",
       " 0.2131309611333383,\n",
       " 0.11293908545899889,\n",
       " 0.7052270180481739]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosin_arr = []\n",
    "\n",
    "for i in range(len(word_1_arr)):\n",
    "    cosin_item = cosin_2_word(word_1_arr[i],word_2_arr[i])\n",
    "    cosin_arr.append(cosin_item)\n",
    "\n",
    "cosin_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pearson (0.3909661429847173, 4.649483750094615e-16)\n",
      " Pearson (0.39094701102518964, 4.666072391253565e-16)\n",
      " Spearman's rank SpearmanrResult(correlation=0.34269861903582854, pvalue=1.82758903036539e-12)\n",
      " Spearman's rank SpearmanrResult(correlation=0.34269861903582854, pvalue=1.82758903036539e-12)\n"
     ]
    }
   ],
   "source": [
    "#For example, to calculate the correlation coefficient between list1 and list2\n",
    "print(\" Pearson\", stats.pearsonr(sim_1_arr,cosin_arr))\n",
    "print(\" Pearson\", stats.pearsonr(sim_2_arr,cosin_arr))\n",
    "\n",
    "print(\" Spearman's rank\", stats.spearmanr(sim_1_arr,cosin_arr))\n",
    "print(\" Spearman's rank\", stats.spearmanr(sim_2_arr,cosin_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ngÆ°á»i_máº¹', 'máº¹', 'anh_trai', 'bá»', 'ngÆ°á»i_cha']\n"
     ]
    }
   ],
   "source": [
    "def k_nearest_words(word_input,k):\n",
    "    \n",
    "#   tinh cosin cua cac tu \n",
    "    list_of_cosin = []\n",
    "    for i in word_arr:\n",
    "        list_of_cosin.append(cosin_2_word(i,word_input))    \n",
    "        \n",
    "#   lay ra k + 1 tu gan nhat\n",
    "    k = k + 1\n",
    "    list_max_index = np.argsort(list_of_cosin)[-k:]\n",
    "#   bo di tu cuoi cung do chinh la tu dang tim kiem\n",
    "    list_max_index = list_max_index[:-1]\n",
    "    \n",
    "#   in ra cac tu trong danh sach\n",
    "    word_max = []\n",
    "    for i in list_max_index:\n",
    "        word_max.append(word_arr[i])\n",
    "    \n",
    "    print(word_max)\n",
    "        \n",
    "k_nearest_words(\"cha\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13558,)\n",
      "(13558, 1)\n",
      "(13558,)\n",
      "(13558, 1)\n",
      "(13558, 300)\n",
      "(13558, 301)\n"
     ]
    }
   ],
   "source": [
    "ant_arr = open(\"./antonym-synonym-set/Antonym_vietnamese.txt\", \"r\")\n",
    "syn_arr = open(\"./antonym-synonym-set/Synonym_vietnamese.txt\",\"r\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "cosin_ant_syn = []\n",
    "\n",
    "for i in syn_arr:\n",
    "    count = count +1\n",
    "    i = i.rstrip().split(\" \")\n",
    "    if len(i) == 2:\n",
    "        cosin_item = cosin_2_word(i[0],i[1])\n",
    "        cosin_ant_syn.append(cosin_item)\n",
    "        \n",
    "        x_item = np.concatenate([embeding_word(i[0]),embeding_word(i[1])])\n",
    "        X.append(x_item)\n",
    "        Y.append(1)\n",
    "        \n",
    "for i in ant_arr:\n",
    "    count = count +1\n",
    "    i = i.rstrip().split(\" \")\n",
    "    if len(i) == 2:\n",
    "        cosin_item = cosin_2_word(i[0],i[1])\n",
    "        cosin_ant_syn.append(cosin_item)\n",
    "        \n",
    "        x_item = np.concatenate([embeding_word(i[0]),embeding_word(i[1])])\n",
    "        X.append(x_item)\n",
    "        Y.append(-1)\n",
    "        \n",
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "cosin_ant_syn=np.array(cosin_ant_syn)\n",
    "print(cosin_ant_syn.shape)\n",
    "cosin_ant_syn = np.reshape(cosin_ant_syn,(-1,1))\n",
    "print(cosin_ant_syn.shape)\n",
    "\n",
    "print(Y.shape)\n",
    "Y = np.reshape(Y,(-1,1))\n",
    "print(Y.shape)\n",
    "\n",
    "print(X.shape)\n",
    "X =  np.concatenate((X, cosin_ant_syn), axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "(1400, 1)\n",
      "(1400, 1)\n",
      "(1400, 301)\n",
      "(1400, 301)\n"
     ]
    }
   ],
   "source": [
    "test_file = open(\"./datasets/ViCon-400/all.txt\", \"r\")\n",
    "count = 0\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "cosin_test = []\n",
    "\n",
    "for i in test_file:\n",
    "    count = count +1\n",
    "    i = i.rstrip().split(\"\\t\")\n",
    "    if len(i) == 3:\n",
    "        cosin_item = cosin_2_word(i[0],i[1])\n",
    "        cosin_test.append(cosin_item)\n",
    "        \n",
    "        x_item = np.concatenate([embeding_word(i[0]),embeding_word(i[1])])\n",
    "        X_test.append(x_item)\n",
    "        \n",
    "        if i[2] == \"ANT\":\n",
    "            Y_test.append(-1)\n",
    "        \n",
    "        if i[2] == \"SYN\":\n",
    "            Y_test.append(1)\n",
    "        \n",
    "print(count)\n",
    "X_test=np.array(X_test)\n",
    "Y_test=np.array(Y_test)\n",
    "Y_test = np.reshape(Y_test,(-1,1))\n",
    "print(Y_test.shape)\n",
    "\n",
    "cosin_test=np.array(cosin_test)\n",
    "cosin_test=np.reshape(cosin_test,(-1,1))\n",
    "print(cosin_test.shape)\n",
    "\n",
    "X_test =  np.concatenate((X_test, cosin_test), axis=1)\n",
    "print(X_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy 0.7271428571428571\n",
      "f1 score 0.7763466042154568\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, Y)\n",
    "mean_accu = clf.score(X_test, Y_test)\n",
    "print(\"mean accuracy\",mean_accu)\n",
    "\n",
    "y_true = Y_test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"f1 score\",f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy 0.9378571428571428\n",
      "f1 score 0.9414141414141414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(gamma='auto')\n",
    "clf_svc.fit(X, Y)\n",
    "\n",
    "print(\"mean accuracy\",clf_svc.score(X_test,Y_test))\n",
    "\n",
    "y_true = Y_test\n",
    "y_pred = clf_svc.predict(X_test)\n",
    "\n",
    "print(\"f1 score\",f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy 0.7028571428571428\n",
      "f1 score 0.7595375722543353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_LinearSVC = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf_LinearSVC.fit(X, Y)\n",
    "\n",
    "print(\"mean accuracy\",clf_LinearSVC.score(X_test,Y_test))\n",
    "\n",
    "y_true = Y_test\n",
    "y_pred = clf_LinearSVC.predict(X_test)\n",
    "\n",
    "print(\"f1 score\",f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='adaptive',\n",
      "              learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy 0.9792857142857143\n",
      "f1 score 0.9796491228070175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP = MLPClassifier(\n",
    "    alpha=1, \n",
    "    max_iter=1000,\n",
    "    hidden_layer_sizes=(100,),\n",
    "    activation=\"relu\",\n",
    "    learning_rate=\"adaptive\"\n",
    ")\n",
    "\n",
    "print(MLP)\n",
    "\n",
    "MLP.fit(X,Y)\n",
    "\n",
    "print(\"mean accuracy\",MLP.score(X_test,Y_test))\n",
    "\n",
    "y_true = Y_test\n",
    "y_pred = MLP.predict(X_test)\n",
    "\n",
    "print(\"f1 score\",f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaf09885471b934d3b8bcc78ae3f91a2e5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
