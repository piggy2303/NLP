{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from scipy.spatial import distance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./word2vec/W2V_150.txt\", \"r\")\n",
    "arr = []\n",
    "for i in f:\n",
    "    arr.append(i.rstrip())\n",
    "\n",
    "word_arr = []\n",
    "vector_arr =[]\n",
    "\n",
    "for i in range(2,len(arr),1):\n",
    "    word = arr[i].split(\"  \")[0]\n",
    "    vector = arr[i].split(\"  \")[1]\n",
    "    vector = vector.split(\" \")\n",
    "    vector = [ float(x) for x in vector ]\n",
    "    \n",
    "    word_arr.append(word)\n",
    "    vector_arr.append(vector)\n",
    "\n",
    "vector_arr = np.array(vector_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sử dụng model w2v để embeding 1 từ\n",
    "\n",
    "Trong model w2v có một vài từ bị thiếu.\n",
    "\n",
    "Do vậy với các từ bị thiếu sẽ sử dụng vector trung bình của tất cả các vector để đại diện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.040183e-01,  1.548617e+00, -1.550459e+00, -1.166396e+00,\n",
       "        9.373734e-01,  5.363901e-01,  9.740896e-01, -4.779322e-01,\n",
       "       -3.882023e-02, -6.313570e-01,  2.814908e-01,  8.697957e-01,\n",
       "        3.346694e-01, -4.105106e-01,  8.294307e-01, -5.471104e-01,\n",
       "       -5.436472e-01,  6.577513e-01, -1.492798e+00,  1.190598e+00,\n",
       "       -3.304991e-02,  4.403790e-01,  3.140532e-01, -2.257410e-01,\n",
       "        5.603623e-01,  6.125256e-01,  4.983729e-01,  4.674868e-01,\n",
       "       -5.752486e-01, -1.862234e+00,  1.277712e-01,  3.462424e-01,\n",
       "       -3.980796e-01, -8.630769e-01,  7.538180e-01,  9.177491e-01,\n",
       "       -9.038814e-01, -2.262156e-01,  1.441921e-01, -5.834721e-01,\n",
       "        3.000867e-01, -7.231166e-02, -2.923416e-01, -7.920831e-01,\n",
       "        5.183015e-01,  9.044061e-01, -7.292989e-01, -7.796840e-01,\n",
       "       -3.591377e-01, -3.016611e-01, -2.381989e-03,  4.421219e-01,\n",
       "        5.601115e-02, -2.657323e-01,  5.161131e-01,  9.320162e-01,\n",
       "        5.678940e-01,  5.736056e-01,  1.282592e+00, -2.892647e-02,\n",
       "        2.695125e-01,  5.353298e-01,  1.156023e-01, -4.829343e-01,\n",
       "       -1.001486e+00, -3.564796e-01,  1.321389e+00, -2.528581e-01,\n",
       "       -7.563136e-01,  2.468856e-01,  9.304204e-02, -5.162832e-01,\n",
       "       -8.049844e-01,  5.163977e-01,  7.717887e-01, -4.885720e-01,\n",
       "       -6.925203e-01, -5.779782e-01, -1.330026e+00, -8.399681e-01,\n",
       "       -8.029588e-01,  1.031780e+00, -3.882761e-01, -5.006179e-02,\n",
       "        5.899854e-01, -8.897933e-01,  4.044803e-01, -1.144684e+00,\n",
       "       -4.818448e-02,  3.498835e-02, -2.857000e-01, -6.831766e-01,\n",
       "        5.838403e-01,  7.715417e-01,  3.493356e-01, -7.866228e-01,\n",
       "        6.383256e-01,  4.529633e-01,  1.529117e-01,  4.647720e-01,\n",
       "       -6.788687e-04, -2.759147e-01, -1.297832e+00, -4.571234e-01,\n",
       "        1.905795e-01, -3.192320e-01, -3.354737e-01, -4.615359e-01,\n",
       "        2.403102e-01,  2.261851e-01, -6.108906e-01, -4.251826e-01,\n",
       "       -7.644030e-01, -5.469632e-01, -4.984001e-01,  6.393032e-01,\n",
       "       -2.611008e-01, -2.740642e-01, -3.668399e-01, -2.337834e-02,\n",
       "       -1.703223e-01,  1.383083e+00,  8.002033e-01, -4.831931e-01,\n",
       "        3.229606e-01,  9.051289e-02,  1.284852e+00, -1.047703e+00,\n",
       "       -1.309618e+00,  3.666870e-01, -4.577653e-01, -4.914412e-01,\n",
       "        1.397826e+00, -6.052859e-01,  5.251831e-01,  1.065836e-01,\n",
       "        1.461753e+00, -4.464338e-01,  1.001367e+00,  1.141722e+00,\n",
       "       -3.723868e-02, -7.768907e-01, -2.754380e-01,  5.597849e-01,\n",
       "        8.027411e-01,  7.033635e-01,  1.923554e+00, -2.286004e-01,\n",
       "        1.157576e-01,  3.278466e-01])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_of_word_unknow = np.mean(vector_arr, axis=0)\n",
    "def embeding_word(word):\n",
    "    try:\n",
    "        return vector_arr[word_arr.index(word)]\n",
    "    except:\n",
    "        return vector_of_word_unknow\n",
    "#         return np.ones(150)\n",
    "    \n",
    "embeding_word(\"tôi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tính toán Cosine similarity giữa 2 từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6105249694717608"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosin_2_word(word1,word2):\n",
    "    word1 = word1.replace(\" \",\"_\")\n",
    "    word2 = word2.replace(\" \",\"_\")\n",
    "    vector1 = embeding_word(word1)\n",
    "    vector2 = embeding_word(word2)\n",
    "    return (1-distance.cosine(vector1,vector2))\n",
    "        \n",
    "cosin_2_word(\"vua\",\"hoàng hậu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "visim_400_file = open(\"./datasets/ViSim-400/Visim-400.txt\", \"r\")\n",
    "visim_arr = []\n",
    "for i in visim_400_file:\n",
    "    visim_arr.append(i.rstrip())\n",
    "\n",
    "visim_arr.pop(0)\n",
    "visim_arr\n",
    "\n",
    "word_1_arr = []\n",
    "word_2_arr = []\n",
    "sim_1_arr = []\n",
    "sim_2_arr = []\n",
    "std_arr=[]\n",
    "\n",
    "for i in visim_arr:\n",
    "    i = i.split(\"\\t\")\n",
    "    word_1_arr.append(i[0])\n",
    "    word_2_arr.append(i[1])\n",
    "    sim_1_arr.append(float(i[3]))\n",
    "    sim_2_arr.append(float(i[4]))\n",
    "    std_arr.append(float(i[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.004912339469669957,\n",
       " 0.08252318329211772,\n",
       " 0.27708595986827755,\n",
       " 0.17679862835626714,\n",
       " 0.2580890432574774,\n",
       " 0.40236612919430603,\n",
       " 0.46300840201407223,\n",
       " 0.2569470088996123,\n",
       " 0.18519202240211619,\n",
       " 0.2230696045254854,\n",
       " 0.5348913029963392,\n",
       " 0.07770108930396213,\n",
       " 0.26146911020272934,\n",
       " 0.07111606873203247,\n",
       " 0.6438835401838533,\n",
       " 0.3610325527728171,\n",
       " 0.059602370849219266,\n",
       " 0.23895503536998697,\n",
       " 0.16647413914742726,\n",
       " -0.17284007209619467,\n",
       " 0.6417888034717881,\n",
       " 0.28753527791057565,\n",
       " 0.38819716944409455,\n",
       " 0.25596472879600973,\n",
       " 0.48043194238556175,\n",
       " 0.4724262988742084,\n",
       " 0.6624093852340639,\n",
       " 0.22523314998167998,\n",
       " 0.17589195648989997,\n",
       " 0.7340683342360695,\n",
       " -0.01360954451948393,\n",
       " 0.6878254420258124,\n",
       " 0.5964044436335486,\n",
       " 0.3754441422124708,\n",
       " 0.3856135822050447,\n",
       " 0.23305646417105508,\n",
       " -0.03025713683863529,\n",
       " 0.09239815053322087,\n",
       " 0.4812901949758728,\n",
       " 0.4430288482597331,\n",
       " 0.42764020562142635,\n",
       " 0.3435372146285758,\n",
       " 0.47775360404332456,\n",
       " 0.16598255399110184,\n",
       " 1.0,\n",
       " 0.3518800216281459,\n",
       " 0.18436466221055814,\n",
       " 0.5372508509460165,\n",
       " 0.20654491300805344,\n",
       " 0.17018185671127573,\n",
       " -0.09567834469126146,\n",
       " 0.4058112852321534,\n",
       " 0.17700723858902045,\n",
       " 0.09295287557078624,\n",
       " 0.6561799539297559,\n",
       " 0.1591479363526458,\n",
       " 0.2573998625832936,\n",
       " 0.2518184385854666,\n",
       " 0.3138728888635596,\n",
       " 0.3079313269934212,\n",
       " 0.14150346138857595,\n",
       " 0.3536777035092057,\n",
       " 0.16630008250549988,\n",
       " 0.2155590695087991,\n",
       " 0.1759844731109964,\n",
       " 0.49028561544494864,\n",
       " 0.12265149004992304,\n",
       " 0.09024219330167871,\n",
       " -0.009955192323517714,\n",
       " 0.18305778374734583,\n",
       " 0.5501337928711713,\n",
       " 0.21009877021166767,\n",
       " 0.05442648829061236,\n",
       " 0.04305196387475296,\n",
       " 0.19459174867201612,\n",
       " 0.5530819388468131,\n",
       " 0.5396216352705175,\n",
       " 0.44477745011189884,\n",
       " 1.0,\n",
       " 0.18252869545755523,\n",
       " 0.1884327286416303,\n",
       " 0.4305675773234774,\n",
       " 0.728347957294409,\n",
       " 0.48128296649134494,\n",
       " 0.06165375134763973,\n",
       " 0.50182926552691,\n",
       " 0.49719492878886484,\n",
       " 0.2640552460341623,\n",
       " -0.008235419222734741,\n",
       " 0.0016070145754620846,\n",
       " 0.3381737506042728,\n",
       " 0.43346360554125285,\n",
       " 0.16435135139844603,\n",
       " 0.3489107092225543,\n",
       " 0.28764478896895396,\n",
       " 0.3992949727831533,\n",
       " 0.27706866745878234,\n",
       " 0.4727960461441949,\n",
       " 0.2604368094017768,\n",
       " 0.3806990925318219,\n",
       " 0.6520561739956902,\n",
       " 0.2429446412966989,\n",
       " 0.39575242059890214,\n",
       " 0.037029307474394146,\n",
       " 0.425593706376884,\n",
       " 0.3002582475972697,\n",
       " 0.117888186841136,\n",
       " 0.11930273869880292,\n",
       " 0.14183917722562245,\n",
       " 0.10001389172491248,\n",
       " -0.17340856215402978,\n",
       " -0.019664365856755905,\n",
       " 0.43308509164042475,\n",
       " 0.23889655843263058,\n",
       " 0.3924741581975577,\n",
       " 0.4791904396921671,\n",
       " 0.2310026808217749,\n",
       " 0.39499832898781273,\n",
       " 0.36495798502767895,\n",
       " 0.5270150819554982,\n",
       " 0.1400762441855683,\n",
       " -0.04568560756811624,\n",
       " -0.04944032913153995,\n",
       " 0.2860688139946971,\n",
       " 0.6348067524315942,\n",
       " 0.5036435709308654,\n",
       " 0.15002384579859185,\n",
       " 0.07522659536547083,\n",
       " 0.181654903465979,\n",
       " 0.15451682858848514,\n",
       " -0.03716774487133834,\n",
       " 0.37012652970055826,\n",
       " 0.28601100050751826,\n",
       " 0.09990871301551096,\n",
       " 0.296198149952809,\n",
       " 0.43431450273571603,\n",
       " 0.11986410627414434,\n",
       " 0.22222659830449643,\n",
       " 0.08176410293427971,\n",
       " 0.2815387476802429,\n",
       " 0.16802562529114884,\n",
       " 0.11699808527980937,\n",
       " -0.01182905114723476,\n",
       " 0.07773202880372376,\n",
       " 0.12892002412737336,\n",
       " 0.22828985638912025,\n",
       " 0.2665077694367288,\n",
       " 0.3117406057866714,\n",
       " 0.557049383355903,\n",
       " 0.3462440983410393,\n",
       " 0.008037526967851982,\n",
       " 0.0420429607431857,\n",
       " 0.21051430924976455,\n",
       " 0.46645676570529293,\n",
       " 0.2602070836579301,\n",
       " 0.46916483596133274,\n",
       " 0.05814091735576743,\n",
       " 0.4903041567774524,\n",
       " 0.2580890432574774,\n",
       " 0.2369407469699243,\n",
       " 0.1438635231004214,\n",
       " 0.5766044993249184,\n",
       " 0.3821435174904365,\n",
       " 0.32032802076548217,\n",
       " 0.5749492271691554,\n",
       " 0.19389669564397183,\n",
       " 0.2416479539708365,\n",
       " 0.5106627798317874,\n",
       " 0.18926458543469316,\n",
       " 0.20940313081938844,\n",
       " -0.029710370501402794,\n",
       " 0.2837016049614861,\n",
       " 0.12074527931992807,\n",
       " 0.017015264264734542,\n",
       " 0.3598694592527043,\n",
       " 0.2651261938064957,\n",
       " 0.3397885209729339,\n",
       " 0.35946235162793316,\n",
       " 0.4219105066378003,\n",
       " 0.8614688965162296,\n",
       " -0.03336303444684785,\n",
       " 0.12090203297460689,\n",
       " 0.31479701144804784,\n",
       " 0.0038128645074774203,\n",
       " 0.058758192228966166,\n",
       " 0.3968391917193709,\n",
       " 0.12428570197414746,\n",
       " 0.19672622271313023,\n",
       " 0.25933155773078487,\n",
       " 0.23267892254977174,\n",
       " 0.05538533879963636,\n",
       " -0.021294972294592096,\n",
       " 0.15980115685287788,\n",
       " 0.14562126981231704,\n",
       " 0.39828525430318873,\n",
       " 0.18902841445299534,\n",
       " 0.3474293225699694,\n",
       " 0.2497490291208706,\n",
       " 0.6089198433181582,\n",
       " 0.6755826366795625,\n",
       " 0.05314055481261115,\n",
       " 0.10054934907463342,\n",
       " 0.3232909354803847,\n",
       " 0.1483878742775997,\n",
       " 0.570699646165023,\n",
       " 0.20133470150686894,\n",
       " -0.005103764065359728,\n",
       " 0.2526270231473787,\n",
       " 0.045425622535701926,\n",
       " 0.1485309587805349,\n",
       " 0.42687936528822523,\n",
       " 0.6700650441380147,\n",
       " 0.08740919551694981,\n",
       " 0.3141633263083572,\n",
       " 0.06264508541390945,\n",
       " 0.28823654399925047,\n",
       " 0.12474992364601212,\n",
       " 0.3452384366954806,\n",
       " 0.10538504266032922,\n",
       " 0.17679103686899356,\n",
       " 0.07106153794008307,\n",
       " -0.08150806029953683,\n",
       " 0.3356669673410032,\n",
       " 0.2332012292070551,\n",
       " 0.22137551879886286,\n",
       " 0.47463479965705146,\n",
       " 0.09329309110204231,\n",
       " 0.3203891892969699,\n",
       " 0.44064836024327436,\n",
       " 0.8422977453212419,\n",
       " 0.49757481319560104,\n",
       " 0.4115581048163304,\n",
       " 0.03402576743432384,\n",
       " 0.0309784792464729,\n",
       " 0.08605219900956884,\n",
       " 0.45581411434867436,\n",
       " 0.07438552352594874,\n",
       " 0.12071931551226445,\n",
       " 0.3909738620022827,\n",
       " 0.22687194401456257,\n",
       " 0.2182430752414123,\n",
       " 0.08913522264813223,\n",
       " 0.2990252010643608,\n",
       " 0.3393090784679019,\n",
       " 0.7741543992805754,\n",
       " 0.3645971891337768,\n",
       " 0.38243149843243907,\n",
       " 0.06499285201502492,\n",
       " 0.041712493805655004,\n",
       " 0.14642503430199727,\n",
       " 0.25190297932721695,\n",
       " 0.18257703468211917,\n",
       " 0.11202609620054438,\n",
       " 0.055979056308360975,\n",
       " 0.018338220644070424,\n",
       " 0.10625734688523503,\n",
       " 0.6423954310858541,\n",
       " 0.5502537205294664,\n",
       " 0.15856256685433023,\n",
       " 0.30474690142029737,\n",
       " 0.35563124319080797,\n",
       " 0.6408591579368886,\n",
       " 0.10812724165852339,\n",
       " 0.5719292130062954,\n",
       " 0.15468965044907834,\n",
       " 0.04145193685409565,\n",
       " 0.10091694820831953,\n",
       " 0.4987182279416986,\n",
       " 0.19975550471725922,\n",
       " 0.11630711350656076,\n",
       " 0.2906393488773251,\n",
       " 0.14819665087209488,\n",
       " 0.10802345785899914,\n",
       " 0.08307659943912193,\n",
       " 0.5963759133200228,\n",
       " 0.40069460609952456,\n",
       " 0.4831838083897517,\n",
       " 0.0869298285463398,\n",
       " 0.4154167385619685,\n",
       " 0.19345520749942513,\n",
       " 0.02805270675860161,\n",
       " 0.30755466764509976,\n",
       " 0.10574808894699206,\n",
       " 0.3174415682631986,\n",
       " 0.09141503336475965,\n",
       " 0.012591047143755474,\n",
       " 0.23699363326882117,\n",
       " 0.0035618958136457435,\n",
       " 0.10090909433832695,\n",
       " 0.10504266823172614,\n",
       " 0.16571494195645498,\n",
       " 0.7283213214108306,\n",
       " 0.687693273029439,\n",
       " 0.5077456024563395,\n",
       " 0.2140835028640664,\n",
       " -0.013589524766159489,\n",
       " 0.5461646248126156,\n",
       " 0.33919365521435485,\n",
       " 0.1212163285207215,\n",
       " 0.3751939726641791,\n",
       " 0.3039713821404869,\n",
       " 0.3458204697154301,\n",
       " 0.06940581189870454,\n",
       " 0.19862973632444048,\n",
       " 0.43775166680046285,\n",
       " 0.4213006540115999,\n",
       " 0.6075960108364697,\n",
       " 0.32489756762342714,\n",
       " 0.1112194069190986,\n",
       " 0.4037583366419032,\n",
       " 0.38944377934967556,\n",
       " 0.7733680092570684,\n",
       " 0.19783202617630358,\n",
       " 0.0943707491060577,\n",
       " 0.5305708447274694,\n",
       " 0.08348869272935444,\n",
       " 0.13855356790222584,\n",
       " 0.16362530474947723,\n",
       " 0.11444190475563798,\n",
       " 0.15933520760572362,\n",
       " 0.15172684174398732,\n",
       " 0.3396688306948408,\n",
       " 0.18473887648929466,\n",
       " 0.45093044850844466,\n",
       " 0.15508711627027516,\n",
       " 0.5284669282716832,\n",
       " 0.1992037674039463,\n",
       " 0.20046350783200317,\n",
       " 0.03124777346515506,\n",
       " 0.44057553236059877,\n",
       " 0.5834579031319869,\n",
       " 0.3150380450984238,\n",
       " 0.13102407921040848,\n",
       " 0.3391485677790276,\n",
       " 0.299426422462176,\n",
       " 0.25047609054912046,\n",
       " -0.11709220841298307,\n",
       " 0.007990260459697529,\n",
       " 0.05150617212689046,\n",
       " 0.12210294605784289,\n",
       " 0.13180365946970285,\n",
       " 0.8179892267593377,\n",
       " 0.15042709634133444,\n",
       " 0.19914344122783612,\n",
       " 0.1480772554325741,\n",
       " 0.1900175709520523,\n",
       " 0.12946731392230693,\n",
       " 0.32445784354190854,\n",
       " -0.0008110941303698382,\n",
       " 0.1844720942077065,\n",
       " 0.16119306934809963,\n",
       " 0.005273336871372658,\n",
       " -0.011613802522149719,\n",
       " 0.15949929905506444,\n",
       " 0.3329660792334814,\n",
       " 0.17995018188629508,\n",
       " 0.25811458928916675,\n",
       " 0.06362501672097443,\n",
       " 0.4074055863436804,\n",
       " 0.5909680719076433,\n",
       " 0.13353498403895525,\n",
       " 0.25218032550202985,\n",
       " 0.4621548455520126,\n",
       " -0.08608104825305274,\n",
       " 0.14359299098434875,\n",
       " 0.9057290306863826,\n",
       " 0.3576096489593774,\n",
       " 0.6325065472381484,\n",
       " 0.6970440205524104,\n",
       " 0.1792190013315711,\n",
       " 0.372503751184449,\n",
       " 0.1642864044630158,\n",
       " 0.2963445022396367,\n",
       " 0.6541988461158454,\n",
       " 0.437246974945656,\n",
       " 0.08713397403587109,\n",
       " 0.3410369530265618,\n",
       " 0.29461659219600633,\n",
       " 0.03854739022579878,\n",
       " 0.20622362094102542,\n",
       " 0.24376703521286547,\n",
       " 0.5151665388049849,\n",
       " 0.4095160578135807,\n",
       " 0.19994660282373244,\n",
       " 0.6742159346587002,\n",
       " 0.2306157487097722,\n",
       " 0.1835720811240651,\n",
       " 0.13696782538735874,\n",
       " 0.1845901522102158,\n",
       " 0.4767118176565459,\n",
       " 0.05671383013487896,\n",
       " 0.470767001732449,\n",
       " 0.4944465344278004,\n",
       " 0.2743758563848939,\n",
       " 0.2580890432574774,\n",
       " 0.17049420746310417,\n",
       " 0.1350079186615406,\n",
       " 0.2131309611333383,\n",
       " 0.11293908545899889,\n",
       " 0.7052270180481739]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosin_arr = []\n",
    "\n",
    "for i in range(len(word_1_arr)):\n",
    "    cosin_item = cosin_2_word(word_1_arr[i],word_2_arr[i])\n",
    "    cosin_arr.append(cosin_item)\n",
    "\n",
    "cosin_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman's rank correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pearson (0.3909661429847173, 4.649483750094615e-16)\n",
      " Pearson (0.39094701102518964, 4.666072391253565e-16)\n",
      " Spearman's rank SpearmanrResult(correlation=0.34269861903582854, pvalue=1.82758903036539e-12)\n",
      " Spearman's rank SpearmanrResult(correlation=0.34269861903582854, pvalue=1.82758903036539e-12)\n"
     ]
    }
   ],
   "source": [
    "#For example, to calculate the correlation coefficient between list1 and list2\n",
    "print(\" Pearson\", stats.pearsonr(sim_1_arr,cosin_arr))\n",
    "print(\" Pearson\", stats.pearsonr(sim_2_arr,cosin_arr))\n",
    "\n",
    "print(\" Spearman's rank\", stats.spearmanr(sim_1_arr,cosin_arr))\n",
    "print(\" Spearman's rank\", stats.spearmanr(sim_2_arr,cosin_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest words\n",
    "### Tìm ra k từ gần với từ được cho "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tướng_quân', 'hoàng_hậu', 'nhà_vua', 'thái_hậu', 'hoàng_đế']\n"
     ]
    }
   ],
   "source": [
    "def k_nearest_words(word_input,k):\n",
    "    \n",
    "#   tinh cosin cua cac tu \n",
    "    list_of_cosin = []\n",
    "    for i in word_arr:\n",
    "        list_of_cosin.append(cosin_2_word(i,word_input))    \n",
    "        \n",
    "#   lay ra k + 1 tu gan nhat\n",
    "    k = k + 1\n",
    "    list_max_index = np.argsort(list_of_cosin)[-k:]\n",
    "#   bo di tu cuoi cung do chinh la tu dang tim kiem\n",
    "    list_max_index = list_max_index[:-1]\n",
    "    \n",
    "#   in ra cac tu trong danh sach\n",
    "    word_max = []\n",
    "    for i in list_max_index:\n",
    "        word_max.append(word_arr[i])\n",
    "    \n",
    "    print(word_max)\n",
    "        \n",
    "k_nearest_words(\"vua\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13558,)\n",
      "(13558, 1)\n",
      "(13558,)\n",
      "(13558, 1)\n",
      "(13558, 300)\n",
      "(13558, 301)\n"
     ]
    }
   ],
   "source": [
    "ant_arr = open(\"./antonym-synonym-set/Antonym_vietnamese.txt\", \"r\")\n",
    "syn_arr = open(\"./antonym-synonym-set/Synonym_vietnamese.txt\",\"r\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "cosin_ant_syn = []\n",
    "\n",
    "for i in syn_arr:\n",
    "    count = count +1\n",
    "    i = i.rstrip().split(\" \")\n",
    "    if len(i) == 2:\n",
    "        cosin_item = cosin_2_word(i[0],i[1])\n",
    "        cosin_ant_syn.append(cosin_item)\n",
    "        \n",
    "        x_item = np.concatenate([embeding_word(i[0]),embeding_word(i[1])])\n",
    "        X.append(x_item)\n",
    "        Y.append(1)\n",
    "        \n",
    "for i in ant_arr:\n",
    "    count = count +1\n",
    "    i = i.rstrip().split(\" \")\n",
    "    if len(i) == 2:\n",
    "        cosin_item = cosin_2_word(i[0],i[1])\n",
    "        cosin_ant_syn.append(cosin_item)\n",
    "        \n",
    "        x_item = np.concatenate([embeding_word(i[0]),embeding_word(i[1])])\n",
    "        X.append(x_item)\n",
    "        Y.append(-1)\n",
    "        \n",
    "X=np.array(X)\n",
    "Y=np.array(Y)\n",
    "cosin_ant_syn=np.array(cosin_ant_syn)\n",
    "print(cosin_ant_syn.shape)\n",
    "cosin_ant_syn = np.reshape(cosin_ant_syn,(-1,1))\n",
    "print(cosin_ant_syn.shape)\n",
    "\n",
    "print(Y.shape)\n",
    "Y = np.reshape(Y,(-1,1))\n",
    "print(Y.shape)\n",
    "\n",
    "print(X.shape)\n",
    "X =  np.concatenate((X, cosin_ant_syn), axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "(1400, 1)\n",
      "(1400, 1)\n",
      "(1400, 301)\n",
      "(1400, 301)\n"
     ]
    }
   ],
   "source": [
    "test_file = open(\"./datasets/ViCon-400/all.txt\", \"r\")\n",
    "count = 0\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "cosin_test = []\n",
    "\n",
    "for i in test_file:\n",
    "    count = count +1\n",
    "    i = i.rstrip().split(\"\\t\")\n",
    "    if len(i) == 3:\n",
    "        cosin_item = cosin_2_word(i[0],i[1])\n",
    "        cosin_test.append(cosin_item)\n",
    "        \n",
    "        x_item = np.concatenate([embeding_word(i[0]),embeding_word(i[1])])\n",
    "        X_test.append(x_item)\n",
    "        \n",
    "        if i[2] == \"ANT\":\n",
    "            Y_test.append(-1)\n",
    "        \n",
    "        if i[2] == \"SYN\":\n",
    "            Y_test.append(1)\n",
    "        \n",
    "print(count)\n",
    "X_test=np.array(X_test)\n",
    "Y_test=np.array(Y_test)\n",
    "Y_test = np.reshape(Y_test,(-1,1))\n",
    "print(Y_test.shape)\n",
    "\n",
    "cosin_test=np.array(cosin_test)\n",
    "cosin_test=np.reshape(cosin_test,(-1,1))\n",
    "print(cosin_test.shape)\n",
    "\n",
    "X_test =  np.concatenate((X_test, cosin_test), axis=1)\n",
    "print(X_test.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy 0.7271428571428571\n",
      "f1 score 0.7763466042154568\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, Y)\n",
    "mean_accu = clf.score(X_test, Y_test)\n",
    "print(\"mean accuracy\",mean_accu)\n",
    "\n",
    "y_true = Y_test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"f1 score\",f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='adaptive',\n",
      "              learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "mean accuracy 0.9785714285714285\n",
      "f1 score 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "MLP = MLPClassifier(\n",
    "    alpha=1, \n",
    "    max_iter=1000,\n",
    "    hidden_layer_sizes=(100,),\n",
    "    activation=\"relu\",\n",
    "    learning_rate=\"adaptive\"\n",
    ")\n",
    "\n",
    "print(MLP)\n",
    "\n",
    "MLP.fit(X,Y)\n",
    "\n",
    "print(\"mean accuracy\",MLP.score(X_test,Y_test))\n",
    "\n",
    "y_true = Y_test\n",
    "y_pred = MLP.predict(X_test)\n",
    "\n",
    "print(\"f1 score\",f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piggy/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy 0.9378571428571428\n",
      "f1 score 0.9414141414141414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(gamma='auto')\n",
    "clf_svc.fit(X, Y)\n",
    "\n",
    "print(\"mean accuracy\",clf_svc.score(X_test,Y_test))\n",
    "\n",
    "y_true = Y_test\n",
    "y_pred = clf_svc.predict(X_test)\n",
    "\n",
    "print(\"f1 score\",f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_LinearSVC = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf_LinearSVC.fit(X, Y)\n",
    "\n",
    "print(\"mean accuracy\",clf_LinearSVC.score(X_test,Y_test))\n",
    "\n",
    "y_true = Y_test\n",
    "y_pred = clf_LinearSVC.predict(X_test)\n",
    "\n",
    "print(\"f1 score\",f1_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaf09885471b934d3b8bcc78ae3f91a2e5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
